{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e1728b",
   "metadata": {},
   "source": [
    "# KNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a2a60",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d64d04",
   "metadata": {},
   "source": [
    "#### Lazy Learning Algorithm\n",
    "- Lazy learning is a type of machine learning where the model doesnt learn a function or build a model from the training data before making predictions.\n",
    "- Lazy learning algorithms store the training data and wait until a query is made to perform any generallisation\n",
    "- When a new data point is intoduced the algorithm, uses the stored data to make a prediction based on the local neighborhood of the input data\n",
    "\n",
    "#### Key Ideas\n",
    "- no training phase,unlike neurall networks LL algorithms do not have a training phase, they only compute when a prediction is needed\n",
    "- lazy learners naturally have low training costs but high prediction costs\n",
    "- since all training data is stored they also require more memory\n",
    "- predictions are based on the local nmeighborhood of the new instance rather than deriving a global model.This makes LLs capable of handling data distribution effectively\n",
    "\n",
    "#### Advantages\n",
    "- Lazy learners can easily adapt to new data since there is not explicit training phase the model can incorporate new data without retraining\n",
    "- They handle complex and irregular decision boundaries well since they rely on local information rather than a global model\n",
    "\n",
    "#### Disadvantages\n",
    "- High predicion cost because whole training dataset must be searched through to make decision\n",
    "- memory intensive for large datasets\n",
    "- Sensitivity to noise because of local data reliance\n",
    "\n",
    "### KNN\n",
    "KNN is a lazy learning algorithm where the classification or prediction of the data point is determined by thre 'k' closese training examples in the feature space.It uses a distance metric to find the closest neighbours.Common distance metrics are the Euclidean distance,Manhattan distance and the Minkowski distance\n",
    "\n",
    "#### Steps in KNN algorithm\n",
    "1. Choose the number of neighbours(k): select an integer k .The optimal value of k can be chosen using techniques such as cross validation\n",
    "2. Comput the distance beteen the new data point and all the training samples\n",
    "3. sort the distances in ascending order\n",
    "4. Select the nearest neighbours: Choose the k nearest data points\n",
    "5. Voting for classification or averaging for regression:\n",
    "    - classification:The new data point is assigned to the class that is most common among its k nearest neighbours(majority voting)\n",
    "    - regression the value of the new data point is the average of sometimes median of the values of its k nearest neighbours\n",
    "    \n",
    "#### Distance Metrics\n",
    "$$\n",
    "\\text{Euclidean Distance:} \\quad d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Manhattan Distance:} \\quad d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Minkowski Distance:} \\quad d(p, q) = \\left(\\sum_{i=1}^{n} |p_i - q_i|^p \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "where p is a new data point and q is the existing data point set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7302d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "####Define Euclidean Distance\n",
    "def euclidean_distance(p,q):\n",
    "    d_squared = np.sum( np.square(p - q) )\n",
    "    d = np.sqrt(d_squared)\n",
    "    return d \n",
    "\n",
    "### Define Manhattan Distance\n",
    "def manhattan_distance(p,q):\n",
    "    d = np.sum(np.abs(p - q))\n",
    "    return d\n",
    "\n",
    "### Define Minkowski Distance\n",
    "def minkowski_distance(p,q):\n",
    "    inside = np.sum(np.abs(p - q)**p)\n",
    "    d = inside ** (1/p)\n",
    "    return d\n",
    "    \n",
    "def F_index(x,y,):\n",
    "    '''Returns the y value corresponding to x value'''\n",
    "    return len\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e9aae99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [1,2,3,4,5,6,7,8,9]\n",
    "p = np.sort(p)\n",
    "p[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69648334",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### KNN algorithm\n",
    "\n",
    "def KNN(k,xpred,x_targ,y_targ,distance='euclidean'):\n",
    "        distances = euclidean_distances(xpred,x_targ)\n",
    "        sorted_distances = np.sort(distances)\n",
    "        \n",
    "        #select k nearest neighbours\n",
    "        KNN = sorted_distances[0:k+1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
