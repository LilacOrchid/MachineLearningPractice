{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fb47e9",
   "metadata": {},
   "source": [
    "# Ensemble Learrning and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410fe61",
   "metadata": {},
   "source": [
    "- Ensemble learning is when you aggregate the predictions oof a group of predictors like c;assifiersn and regressors you will often get a better rprediction with this than the ebst individual predictor \n",
    "- a grroup of prdictors is called an ensembklle so so this technique is called ensemble learning and the algorithm is called an ensemple ,ethjd\n",
    "- for example you can traing a ggroup of decision tree classifiers on a different random subset of the training set . To make predictions yoiu get all the predictions of the indiivual trees then predict the class that getrrs the most votes .\n",
    "- the ensemblle of decision trees is called a reandom forest this is one of the most powerful machine learning algorrithms\n",
    "- so ensemble methods are used at the endd of a project when you built a few good predictors to combvine them into a better predictor \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54fe14",
   "metadata": {},
   "source": [
    "#### vOTING Classifiers\n",
    "- if we tain a load of classifiers and they all have a good accurrac a siumpkle way to get an even betterr classifier is to aggregate the predictions of each classifier and prrediuct the class that get the most vots\n",
    "- the majorit botring classifier is called a hard voting classifier\n",
    "- this votriung clasifier is higher accuracy because of the lalw of larrhe if we had 1000 classifierrs that are onl correct 51 percent of the time so barel better that random huessing iofg we prerdict the ,ajoritty vvoted class we can expect 75 perrcent accuracy.\n",
    "- this is tue io all the classifierrrs are perfectl independednt making uncorrelated errorrrsbut in reality they wont be perfect; independent\n",
    "- ensemble methods work best when the predictors are as independent from one another as  possible for exmaple trainign them, using vbery differrent algorithms so they all make difffernt types of erros improvingoveralll esemble acduracy\n",
    "8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b8480",
   "metadata": {},
   "source": [
    "#### Lets train an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd41446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moons Data\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y= make_moons(n_samples=1000,noise=0.6)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1aa0b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression()),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;svc&#x27;, SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression()),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier()), (&#x27;svc&#x27;, SVC())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f959e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.775\n",
      "RandomForestClassifier 0.765\n",
      "SVC 0.775\n",
      "VotingClassifier 0.795\n"
     ]
    }
   ],
   "source": [
    "#check accurracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d7752",
   "metadata": {},
   "source": [
    "- Ass you can see the ensemble voting classifier is the most accurate\n",
    "- soft votiung is when is the classifier provide sprobability estimates such as the SVC class we can switch toi oft voting which is when the predictions are weighted by the probabilit esditmnates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904e60b",
   "metadata": {},
   "source": [
    "#### Baggin and pasting\n",
    "- we cxan het a diverse set of classifier using different trrainign algorithms\n",
    "- another apprreach is to use the same training algotith for every prredictor and train them on different random subnsets of the trainign set \n",
    "- when the sampling is performed iwht replacement this method is called baggin \n",
    "-- if there is no replacement itnis called pasting\n",
    "- both baggin and pasig allow training instances to sbe sampkled sevberal tims across multiple predicots biut l baggin allopwas training instances to be samples several times for the same prredictorr \n",
    "- once all the rrpedictorrs are trained the ensemnle can ,ake a prediction for a new instance by aggrregating the prediciton pf all rpedictorr s\n",
    "- the aggrrergation fuincvtion is useually the statisitcal mode ioe the most frewuent prrediction or the average for regresiiom\n",
    "- each individual prredictor has a higher bnias than were trained on the rginal trraining set but aggrergation reduces bnoth bias and variance \n",
    "- ensemble has a similarr bias but lower variance than a simgle predictorr trained on the orrignal trrainin set\n",
    "- baggin and pastin scale very well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39580d73",
   "metadata": {},
   "source": [
    "# baggin and pasting in Sklearn\n",
    "- sklearn offers an API for baggin asn pasting with the `BaggingClassifier` class or `BaggingRegression` for regrerssion.The code trains on an ensemble of 500 Decision Tree classifierrs each trained on a 1== trrainign instances randoml;y sampled from the training set with replacement ie baggin\n",
    "- if you want to use pasting set `bootstrap=False`\n",
    "- The n_jobs paraemter telll sklearn the numbner of CPU cores to use\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a41365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),n_estimators=500,\n",
    "    max_samples=100,bootstrap=True,n_jobs=-1)\n",
    "    \n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75707d6",
   "metadata": {},
   "source": [
    "Bootstrapping  introfduced more diverrsity in the sibsets that each orredictor ins trained on. so baggin has a slightl hier bias than asting buit the extra diversity ,eanms they are less correellated ,e,and the ensemble variance is refuce d\n",
    "baggin oftern rresults in better models but if there is m,ore time and CPU [\n",
    "opwer oi can use cross validation to check borth methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e45144",
   "metadata": {},
   "source": [
    "#### Out of bag evaluation\n",
    "the oob instances are the data instances that are not sampled at all with replacments \n",
    "- a bangin ensemple can be evaluate siusing oob instances wihtout the need for a serpate validation set so of there are lot of estimators the each instance of ahte traning set will likely be an oob instance ovf several extimators so these estimatorrs can be used to make afair exembke rpediction forr that instance\n",
    "- we can do this b setting `oob_score = True` by creating a BaggingClassifier to request an automatic oob evaluation after trraining,,\n",
    "The foloowing code demostrates this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb41263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),n_estimators=500,\n",
    "    bootstrap=True,n_jobs=-1,oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ba041",
   "metadata": {},
   "source": [
    "This means the baggin classifer is likel to achieve round 75 percent on the test set from validating o hte oobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38da4b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.765"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ecfeb",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "This is an ensemble of Decition Trees which arre generallyy trained with the baggin method with the max_samples set to the size of the trainign set\n",
    "0 we can esilt implement this isogm the randomforest classifier class instead of crreating a baggin classifier and passing decisiont rees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bca97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=-1)\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aff708",
   "metadata": {},
   "source": [
    "The RANDOM FOREST CLASSIFER has all the hyper params of the decvision trree classifier and the bagign classifeir\n",
    "- the alfotihm introduces extera randomess when grwing trees . it doesnt search for the est features when splitting nodes it seaches for the ebst feature amond a random subset of feateues \n",
    "- this fgive betterr tree diversity and trades higher bias for a lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8339b",
   "metadata": {},
   "source": [
    "#### Ectra Trees\n",
    "- this is when ou make the tress more random by using random thrreshold dfor reach dfeature trather thans earrhing for the best possible threshold \n",
    "- we call theis ectrremely randomised trees \n",
    "- this is much gadter than a nojmral random forest \n",
    "- call the `ExtraTreesClassifier`\n",
    "- usuall you test bnoth random forests und ciomoare them using cross validation and fgrid seaarch to determin whichn is better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90143f83",
   "metadata": {},
   "source": [
    "#### Random forests can show featurre importance \n",
    "- sklearn measures a featruer imporance by looking how much thrrre tree nodes that use that featurrre reduce impurit on averrage across all trees in the forest ,, ie its a weigherd abergae where each nodes weight is equal tot he number of trainign samples thhat are associate with it \n",
    "- sklearn comuites this score automaticall afeter trrainign the scaled is so the sum is 1 \n",
    "\n",
    "we do this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "553dd578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09859282149466389\n",
      "sepal width (cm) 0.023595486055755205\n",
      "petal length (cm) 0.428483564010211\n",
      "petal width (cm) 0.44932812843937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500,n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'],iris['target'])\n",
    "for name, score in zip(iris['feature_names'],rnd_clf.feature_importances_):\n",
    "    print(name,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1712b8c",
   "metadata": {},
   "source": [
    "You can see clearly the peral legnth and width are the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f276d2",
   "metadata": {},
   "source": [
    "#### Boosting\n",
    "- This is any ensemble method that cominies several weak learners intoa strong learner\n",
    "- The gerna;l idea is to train predictors sequentiall, each try to correct its predecessor\n",
    "- The most po;ul;at are Ada Boost and Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d8091",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "one way fo the new predictor to correct its predecessor is tio pay a bit more attention to the training instances that the predeccsirrre undersfitted \n",
    "- this results in a enw predictors focusing morre and more on the ahrrd cases \n",
    "- this is the technique use dby ADA boost \n",
    "- when trianign an ADABoost classifier thenalforith tains a base classfier like a decision tree and uses it to make prediction on the trraining set \n",
    "- the algo then increraes the relativwe weight of the misclassified training instances\n",
    "- it now uses a second classifier useing the iopdated trainign weight and make new predictions-\n",
    "- this now cares on\n",
    "\n",
    "- lets say the first classifier gets many instacnes wrong so teir weights get boosted the secoind classifier does a better job on these instance ans so on \n",
    "\n",
    "- its like gradient decent but it addes predictors to the esembkle making it better ather than minimuising a cost function by teweaking params\n",
    "\n",
    "- once all the predictors are trained the ensembkle makes predictions like bagiin orr pasting but the predictors havb different weigfhts depending on their overall accuracy on the weighted trraining set \n",
    "- uncoftunatel as its sequential learrning it cannot be pareelerised since reach prredictor strianaes in sequence so clasing i not as good as baaggin or pasting\n",
    "- Skleasrn used a multioclass version of AdaBoost called SAMME if the perdicoprs c can estimate class probs we can use SAMME.R which is ghenerall better \n",
    "- tghe code beow traines the AdaBoost classifier using 200 decisoin stuimps shich is a decision tree iwth a max depth of 1 m, this is aso the defauilt base estimator for ther ada boos classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db39a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),n_estimators=200,\n",
    "    algorithm='SAMME.R',learning_rate=0.5) #algorithmmis the boosting method for weak learners\n",
    "ada_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ce3eb",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7771a4c",
   "metadata": {},
   "source": [
    "-Grradient boosting works by sequentially adding predictors to the ensemble each one correcting its predessor but instead of tweaking the instance weights at every iteration like Ada it tries to fit a new prredictor to the rrersidual erros made b the pevious prredictor \n",
    "if we use decision trees and the base we can call it gradient rree boosting orr grradient boosted regression trees \n",
    "- lets fit a decision tree regressor to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea628ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXAMPLE CODE FOR SYNTAX\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X_train,y_train)\n",
    "#now train a second decision tree on the rrecidual eros made bt the firs predictor\n",
    "\n",
    "y2 = y_train - tree_reg1.predict(X_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X_train,y2)\n",
    "\n",
    "# now we train a thirrd regrssor on the seond residuals \n",
    "y3 = y2 - tree_reg2.predict(X_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X_train,y3)\n",
    "\n",
    "# now we have an esemblke conatining three trees it can make predictions \n",
    "#on a new instance b additng ip the rpedcition of all the trees\n",
    "\n",
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1,tree_reg2,tree_reg3)) \n",
    "## we sum as the models adjusting the resiudals for places wherre it perfformed badly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904cd9ff",
   "metadata": {},
   "source": [
    "wE Can implement the gradient boosint rregressorr quinck;y using the code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a226b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=1, max_depth=2, n_estimators=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=1, max_depth=2, n_estimators=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e56b2",
   "metadata": {},
   "source": [
    "in order to diuns the optimal number of trrees ou can use earrl stopping using the `staged_predict()` method\n",
    "- it returns an iterrator over the precitions mad ebyt the ensembnle at each staghe of training ie with one tree two trees etc \n",
    "- the following code trrains the GBRRT ensemble to fin the optimal number of trees then train=s another GBRRT using the optimal nubner of rtrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982cb2e",
   "metadata": {},
   "source": [
    "```import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=120)\n",
    "gbrt.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "errors = [mean_squared_error(y_train,y_pred)\n",
    "         for y_prerd in gbrt.staged_predict(X_test)]\n",
    "bst_n_estimators = np.argmin(errors)+1\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train,Y_train)```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262ad6d",
   "metadata": {},
   "source": [
    "#### Stacking\n",
    "- this is the case when instead of using functions such as hard or soft voting we can acutallly tain a model to perform the aggrreghating \n",
    "- the dinal predictor is called the blender or meta learner\n",
    "- to trin the blender a commoon aprroach is to use a holld out set \n",
    "- the training set is spliot intop two subsets the firrst subset is used to tain predictorrs in the firs tlayer \n",
    "- the firtst layer predictorrs are now iused to maek mprredictions on the second holdout set \n",
    "- this ensures th predictions are clean and since the predictos never saw these isntances during tainign each instance in the hold out set there arre now predictions for each predictor used \n",
    "- we can crreate a new training set using thes predictesd values as input ffeatures and keeping the same starrget calues \n",
    "- the blender is trained on this new trainin set so it learns to predict the tarrrget calue fiven the first layerrs predictions\n",
    "- we can furthe rthis by acxtuall training several blenders using different predictors to get a whole layer of blanders - sklearn does nt support stacking directl but it can be implemented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d966b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
